#+Title: Probability and Statistic 
#+AUTHOR: Lucas Peng
#+EMAIL: lucasp0927@gmail.com
#+INCLUDE: header/header.org org
#+STARTUP: showall
#+OPTIONS: toc:3
#+LINK-HOME: index
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{amsmath}

* Basics
** Axiom
1. For any event $A$, $P[A]\geq 0$
2. $P[S]=1$
3. For any countable collection $A_1,A_2$ ... of mutually exclusive events \[P[A_1\cup A_2\cup ...]=P[A_1]+P[A_2]+...\]
** Bayes' Theorem
\[P[B|A]=\frac{P[A|B]P[B]}{P[A]}\]

\[P[B_i|A]=\frac{P[A|B_i]P[B_i]}{\sum^m_{i=1}P[A|B_i]P[B_i]}\]
** Expected Value
\[E[X]=\mu_X=\sum_{x\in S_X}{}{xP_X(s)}\]
*** Little Trick
\[E\left[k^2\right]=E[k(k-1)]+E[k]\]
*** Another Trick
\[E[X]=\int^\infty_0[1-F_X(x)]\;dx\]
** Derived Random Variable
\[P_{Y}(y)=\sum_{x:g(x)=y}{P_{X}(x)}\]

\[E[Y]=\mu_{Y}=\sum_{x\in S_{X}}{g(x)P_{X}(s)}\]

\[E[aX+b]=aE[X]+b\]
** Variance
\[\textrm{Var}[X]=E\left[(X-\mu_X)^2\right]\]

\[\textrm{Var}[X]=E\left[X^2\right]-(E[X])^2\]

\[\textrm{Var}[aX+b]=a^{2}\textrm{Var}[X]\]
** Conditional PMF
\[P_{X|B}(x)=\begin{cases}
\frac{\displaystyle P_X(x)}{\displaystyle P[B]} & x\in B \\
0 & \textrm{otherwise}\end{cases}\]
* Discrete Random Variables
** Bernoulli Random Variable
Bernoulli random variable is a single test if an event is going to happen.

\[P_{X}(x)=\begin{cases}
1-p & x=0\\
p & x=1\\
0 & \textrm{otherwise}\end{cases}\]

For $0<p<1$.

\[E[X]=p\]

\[\textrm{Var}[X]=p(1-p)\]
** Geometric Random Variable
Geometric random variable describes the probability of $x-1$ continuous success with the last one failure.

\[P_{X}(x)=\begin{cases}
p(1-p)^{x-1} & x=1,2,...\\
0 & \textrm{otherwise}\end{cases}\]

For $0<p<1$.

\[E[X]=1/p\]

\[\textrm{Var}[X]=(1-p)/p^2\]
** Binomial Random Variable
Binomial random variable tells us when there are $n$ tests, what is the probabilty of $x$ tests are failure?

\[P_{X}(x)=\binom{n}{x}p^x(1-p)^{n-x}\]

For $0<p<1$ and $n\geq1$.

\[E[X]=np\]

\[\textrm{Var}[X]=np(1-p)\]
** Pascal Random Variable
Pascal random variable is the probability of $k$ failures at the $x$ th test.

\[P_{X}(x)=\binom{x-1}{k-1}p^k(1-p)^{x-k}\]

For $0<p<1$ and $k\geq1$.

\[E[x]=k/p\]

\[\textrm{Var}[X]=k(1-p)p^2\]
** Poisson Random Variable
Let $\lambda$ is the arrival rate (1/T), and $T$ is the time interval.  Then $\alpha=\lambda T$ is the total arrivals, Poisson random varible shows the probability of $x$ arrivals.

\[
P_{X}(x)=\begin{cases}
\alpha^{x}e^{-\alpha}/x! & x=0,1,2,...\\
0 & \textrm{otherwise}\end{cases}\]

For $\alpha >0$.

\[E[X]=\alpha\]

\[\textrm{Var}[X]=\alpha\]
** Discrete Uniform Random Variable
\[
P_{X}(x)=\begin{cases}
1/(l-k+1) & x=k,k+1,k+2,...,l \\
0 & \textrm{otherwise}\end{cases}\]

\[E[X]=(k+l)/2\]

\[\textrm{Var}[X]=(l-k)(l-k+2)/12\]

* Continuous Random Variables
** Uniform Random Variable
PDF:\\
\[
f_X(x)=\begin{cases}
1/(b-a) & a \leq x < b \\
0 & \textrm{otherwise}
\end{cases}
\]

CDF:\\
 \[
F_X(x)=\begin{cases}
0 & x \leq a \\
(x-a)/(b-a) & a < x\leq b \\
1 & x>b
\end{cases}
\]

\[E[X]=(b+a)/2\]

\[\textrm{Var}[X]=(b-a)^2/12\]

** Exponential Random Variable
PDF:\\
\[f_X(x)=\begin{cases}
\lambda e^{-\lambda x} & x \geq 0 \\
0 & \textrm{otherwise}\end{cases}\]

For $\lambda > 0$.

CDF:\\
\[F_X(x)=\begin{cases}
1-e^{-\lambda x} & x \geq 0\\
0  & \textrm{otherwise}\end{cases}\]

\[E[X]=1/\lambda\]

\[\textrm{Var}[X]=1/\lambda^2\]

*** Memoryless property
\[f_T(t-n)=f_{T|T>n}(t)\]

\[E[T|T>n]=E[T]+n\]
** Erlang Ramdom Variable
PDF:\\
\[f_X(x)=\begin{cases}
\frac{\displaystyle \lambda^{n}x^{n-1}e^{-\lambda x}}{\displaystyle (n-1)!} & x \leq 0 \\
0 & \textrm{otherwise}\end{cases}\]

For $\lambda > 0$ and $n \geq 1$, $n \in \mathbb{N}$.

\[E[X]=n/\lambda\]

\[\textrm{Var}[X]=n/\lambda^2\]

** Gaussian Random Variable
PDF:\\
\[f_X(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2}\]

For $\sigma > 0$.

\[E[X]=\mu\]

\[\textrm{Var}[X]=\sigma^2\]
*** Functions related to normal distribution
Standard normal CDF:\\
\[\Phi(z)=\frac{1}{\sqrt{2\pi}}\int^z_{-\infty}e^{-u^2/2}du\]

\[F_X(x)=\Phi\left(\frac{x-\mu}{\sigma}\right)\]

Standard normal complementary CDF:\\
\[Q(z)=\frac{1}{\sqrt{2\pi}}\int^{\infty}_z{e^{-u^2/2}du}\]

* Pairs of Random Variables
** Definition
\[\textrm{Cov}[X,Y]=E[(X-\mu_X)(Y-\mu_Y)]\]

\[r_{X,Y} = E[XY]\]

\[\rho_{X,Y}=\frac{\textrm{Cov}[X,Y]}{\sqrt{\textrm{Var}[X]\textrm{Var}[Y]}}\]

** Facts
\[E[X+Y]=E[X]+E[Y]\]

\[\textrm{Var}[X+Y]=\textrm{Var}[X]+\textrm{Var}[Y]+2\textrm{Cov}[X,Y]\]

\[\textrm{Cov}[X,Y]=r_{X,Y}-\mu_X \mu_Y\]
** Conditioning
\[\textrm{Var}[W|B]=E\left[\left(W-\mu_{W|B}\right)^2|B\right]=E\left[W^2|B\right]-(\mu_{W|B})^2\]

\[E[E[X|Y]]=E[X]\]

** Independent

\[\textrm{Cov}[X,Y]=\rho_{X,Y}=0\]

\[E[XY]=E[X]E[Y]\]

** Bivariate Gussian Random Variables
\[f_{X,Y}(x,y)=\frac{e^{\displaystyle{-\frac{\left(\frac{x-\mu_1}{\sigma_1}\right)^2-\frac{2\rho(x-\mu_1)(y-\mu_2)}{\sigma_1 \sigma+2}+\left(\frac{y-\mu_2}{\sigma_2}\right)^2}{2(1-\rho^2)}}}}{2\pi\sigma_1 \sigma_2\sqrt{1-\rho^2}}\]
------
[[file:index.org][Back to Index]]
